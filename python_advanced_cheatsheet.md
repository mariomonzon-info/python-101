Comenzamos el **Nivel Avanzado** con uno de los frameworks mÃ¡s modernos, rÃ¡pidos y eficientes para desarrollo web en Python: **FastAPI**.

---

# ğŸš€ MÃ³dulo 1 â€“ IntroducciÃ³n a FastAPI y desarrollo backend profesional

---

## ğŸ¯ Objetivos del mÃ³dulo

* Comprender los fundamentos de FastAPI.
* Crear una API RESTful moderna con endpoints funcionales.
* Validar datos con **Pydantic**.
* Conectar la API a una base de datos con **SQLModel** o **SQLAlchemy**.
* Usar **OpenAPI/Swagger** para documentar automÃ¡ticamente la API.
* Aplicar buenas prÃ¡cticas para proyectos backend en producciÃ³n.

---

## ğŸ§° Â¿QuÃ© es FastAPI?

FastAPI es un framework web **asÃ­ncrono y moderno** para crear APIs con Python 3.7+ basado en:

* **Pydantic** para validaciÃ³n de datos.
* **Starlette** como motor web.
* DocumentaciÃ³n automÃ¡tica con **Swagger UI** y **ReDoc**.

### ğŸ”¥ Ventajas:

* MÃ¡s rÃ¡pido que Flask.
* ValidaciÃ³n automÃ¡tica de datos.
* Tipado fuerte con anotaciones de Python.
* Muy fÃ¡cil de testear y escalar.

---

## ğŸ› ï¸ Requisitos previos

Instala las dependencias bÃ¡sicas:

```bash
pip install fastapi uvicorn
```

Para trabajar con bases de datos:

```bash
pip install sqlmodel[all]  # o puedes usar sqlalchemy + pydantic si prefieres
```

---

## ğŸ§ª Proyecto base: API de libros (versiÃ³n RESTful)

Creamos un proyecto que expone endpoints para:

* Crear libros
* Obtener todos los libros
* Consultar libro por ID
* Eliminar libros
* Actualizar libros

---

## ğŸ“ Estructura del proyecto

```bash
fastapi_libros/
â”œâ”€â”€ main.py
â”œâ”€â”€ models.py
â”œâ”€â”€ database.py
â”œâ”€â”€ crud.py
â”œâ”€â”€ schemas.py
â”œâ”€â”€ test_main.py
â””â”€â”€ requirements.txt
```

---

## ğŸ“„ `models.py` (modelo + validaciÃ³n con SQLModel)

```python
from sqlmodel import SQLModel, Field
from typing import Optional

class Libro(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    titulo: str
    autor: str
    anio: int
```

---

## ğŸ“„ `schemas.py` (DTOs para entrada/salida)

```python
from pydantic import BaseModel

class LibroCreate(BaseModel):
    titulo: str
    autor: str
    anio: int
```

---

## ğŸ“„ `database.py` (conexiÃ³n y creaciÃ³n de tablas)

```python
from sqlmodel import create_engine, SQLModel, Session

DATABASE_URL = "sqlite:///./libros.db"
engine = create_engine(DATABASE_URL, echo=True)

def crear_tablas():
    SQLModel.metadata.create_all(engine)

def get_session():
    with Session(engine) as session:
        yield session
```

---

## ğŸ“„ `crud.py` (operaciones de base de datos)

```python
from sqlmodel import Session, select
from models import Libro

def crear_libro(session: Session, libro: Libro):
    session.add(libro)
    session.commit()
    session.refresh(libro)
    return libro

def obtener_libros(session: Session):
    return session.exec(select(Libro)).all()

def obtener_libro(session: Session, libro_id: int):
    return session.get(Libro, libro_id)

def eliminar_libro(session: Session, libro_id: int):
    libro = session.get(Libro, libro_id)
    if libro:
        session.delete(libro)
        session.commit()
        return True
    return False
```

---

## ğŸ“„ `main.py` (servidor principal)

```python
from fastapi import FastAPI, Depends, HTTPException
from sqlmodel import Session
from models import Libro
from schemas import LibroCreate
from database import crear_tablas, get_session
import crud

app = FastAPI(title="API de Libros")

@app.on_event("startup")
def on_startup():
    crear_tablas()

@app.post("/libros/", response_model=Libro)
def crear(libro_data: LibroCreate, session: Session = Depends(get_session)):
    libro = Libro(**libro_data.dict())
    return crud.crear_libro(session, libro)

@app.get("/libros/", response_model=list[Libro])
def listar(session: Session = Depends(get_session)):
    return crud.obtener_libros(session)

@app.get("/libros/{libro_id}", response_model=Libro)
def obtener(libro_id: int, session: Session = Depends(get_session)):
    libro = crud.obtener_libro(session, libro_id)
    if not libro:
        raise HTTPException(status_code=404, detail="Libro no encontrado")
    return libro

@app.delete("/libros/{libro_id}")
def borrar(libro_id: int, session: Session = Depends(get_session)):
    if not crud.eliminar_libro(session, libro_id):
        raise HTTPException(status_code=404, detail="Libro no encontrado")
    return {"ok": True}
```

---

## ğŸš€ Ejecutar el servidor

```bash
uvicorn main:app --reload
```

Accede a la documentaciÃ³n interactiva en:

* ğŸ“˜ Swagger UI: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
* ğŸ“• ReDoc: [http://127.0.0.1:8000/redoc](http://127.0.0.1:8000/redoc)

---

## âœ… Buenas prÃ¡cticas

* Divide cÃ³digo en modelos, esquemas, base de datos y lÃ³gica (CRUD).
* Usa `Depends()` para inyecciÃ³n de dependencias (como la sesiÃ³n).
* Valida siempre los datos con Pydantic.
* Usa OpenAPI y documentaciÃ³n integrada como estÃ¡ndar de desarrollo.

---

## ğŸ“ Ejercicios sugeridos

1. AÃ±adir endpoint `PUT /libros/{id}` para actualizar un libro.
2. AÃ±adir campo booleano `prestado` al modelo con endpoint para marcarlo como prestado/devuelto.
3. Implementar paginaciÃ³n (`limit`, `offset`) en el `GET /libros/`.
4. Agregar filtros por autor o aÃ±o.

---

Perfecto. Entramos en una de las partes mÃ¡s importantes para cualquier backend profesional: **AutenticaciÃ³n y autorizaciÃ³n**, usando **FastAPI con JWT y OAuth2**.

---

# ğŸ” MÃ³dulo 2 â€“ AutenticaciÃ³n y autorizaciÃ³n en FastAPI (JWT, OAuth2, usuarios y roles)

---

## ğŸ¯ Objetivos del mÃ³dulo

* Implementar **login seguro** usando tokens JWT.
* Proteger rutas con **OAuth2 Password Flow**.
* Crear y gestionar usuarios y contraseÃ±as en la base de datos.
* Implementar **roles y permisos** bÃ¡sicos.
* Aplicar buenas prÃ¡cticas de seguridad en APIs web.

---

## ğŸ› ï¸ LibrerÃ­as necesarias

```bash
pip install passlib[bcrypt] python-jose[cryptography]
```

---

## ğŸ“ Estructura del proyecto

```bash
fastapi_auth/
â”œâ”€â”€ main.py
â”œâ”€â”€ auth.py         # lÃ³gica de autenticaciÃ³n y JWT
â”œâ”€â”€ models.py       # modelos SQLModel
â”œâ”€â”€ schemas.py      # DTOs
â”œâ”€â”€ database.py
â”œâ”€â”€ users.py        # rutas protegidas
â””â”€â”€ test_auth.py
```

---

## ğŸ” Fundamentos de JWT

Un **JWT (JSON Web Token)** contiene:

* **Header**: algoritmo (ej. HS256)
* **Payload**: datos codificados (ej. user\_id, rol)
* **Signature**: asegura que el contenido no ha sido alterado

---

## ğŸ“„ `models.py`

```python
from sqlmodel import SQLModel, Field
from typing import Optional

class Usuario(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    username: str
    email: str
    hashed_password: str
    rol: str = "usuario"
```

---

## ğŸ“„ `schemas.py`

```python
from pydantic import BaseModel

class UsuarioCreate(BaseModel):
    username: str
    email: str
    password: str

class UsuarioOut(BaseModel):
    id: int
    username: str
    email: str
    rol: str

class Token(BaseModel):
    access_token: str
    token_type: str
```

---

## ğŸ“„ `auth.py`

```python
from datetime import datetime, timedelta
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from sqlmodel import Session, select
from models import Usuario
from database import get_session

# Clave secreta (usa una segura en prod)
SECRET_KEY = "supersecreta"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="login")

def verificar_password(plain, hashed):
    return pwd_context.verify(plain, hashed)

def hash_password(password):
    return pwd_context.hash(password)

def crear_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def verificar_token(token: str = Depends(oauth2_scheme), session: Session = Depends(get_session)):
    cred_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="No autorizado",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise cred_exception
    except JWTError:
        raise cred_exception

    user = session.exec(select(Usuario).where(Usuario.username == username)).first()
    if user is None:
        raise cred_exception
    return user
```

---

## ğŸ“„ `main.py` (registro y login)

```python
from fastapi import FastAPI, Depends, HTTPException
from database import crear_tablas, get_session
from sqlmodel import Session
from models import Usuario
from schemas import UsuarioCreate, UsuarioOut, Token
from auth import hash_password, verificar_password, crear_token
from fastapi.security import OAuth2PasswordRequestForm

app = FastAPI()

@app.on_event("startup")
def startup():
    crear_tablas()

@app.post("/registro", response_model=UsuarioOut)
def registrar(usuario: UsuarioCreate, session: Session = Depends(get_session)):
    user = Usuario(
        username=usuario.username,
        email=usuario.email,
        hashed_password=hash_password(usuario.password)
    )
    session.add(user)
    session.commit()
    session.refresh(user)
    return user

@app.post("/login", response_model=Token)
def login(form_data: OAuth2PasswordRequestForm = Depends(), session: Session = Depends(get_session)):
    user = session.exec(select(Usuario).where(Usuario.username == form_data.username)).first()
    if not user or not verificar_password(form_data.password, user.hashed_password):
        raise HTTPException(status_code=400, detail="Credenciales incorrectas")
    
    token = crear_token({"sub": user.username})
    return {"access_token": token, "token_type": "bearer"}
```

---

## ğŸ“„ `users.py` (rutas protegidas con roles)

```python
from fastapi import APIRouter, Depends, HTTPException
from auth import verificar_token
from models import Usuario

router = APIRouter()

@router.get("/me")
def obtener_usuario_actual(user: Usuario = Depends(verificar_token)):
    return user

@router.get("/admin")
def solo_admin(user: Usuario = Depends(verificar_token)):
    if user.rol != "admin":
        raise HTTPException(status_code=403, detail="Acceso restringido a administradores")
    return {"mensaje": "Bienvenido, administrador"}
```

Y luego en `main.py`:

```python
from users import router as user_router
app.include_router(user_router)
```

---

## ğŸ” Probar en Swagger

1. RegÃ­strate con `/registro`.
2. Haz login en `/login` â†’ copia el token.
3. Pulsa "Authorize" en Swagger y pega `Bearer <tu_token>`.
4. Accede a `/me` o `/admin`.

---

## âœ… Buenas prÃ¡cticas

* Guarda la `SECRET_KEY` y `DATABASE_URL` en variables de entorno.
* Aplica hashing seguro con `bcrypt`.
* Define funciones reutilizables para autenticaciÃ³n y autorizaciÃ³n.
* Usa decoradores o dependencias para controlar roles (ej. `admin_required`).
* No expongas contraseÃ±as en las respuestas.

---

## ğŸ“ Ejercicios prÃ¡cticos

1. AÃ±adir endpoint para cambiar la contraseÃ±a del usuario autenticado.
2. AÃ±adir campo `activo: bool` y evitar login si `False`.
3. Implementar funciÃ³n `@admin_required` como decorador o dependencia.
4. Crear endpoint que liste todos los usuarios, solo accesible para admins.

---

Perfecto. Vamos con un mÃ³dulo muy Ãºtil para aplicaciones web completas: formularios, validaciÃ³n avanzada y gestiÃ³n de archivos, incluyendo **subida de imÃ¡genes**.

---

# ğŸ—‚ï¸ MÃ³dulo 3 â€“ Subida de archivos, formularios y validaciÃ³n avanzada en FastAPI

---

## ğŸ¯ Objetivos del mÃ³dulo

* Subir y guardar archivos en el servidor (imÃ¡genes, PDFs, etc.).
* Procesar formularios HTML con `Form`.
* Validar inputs usando expresiones regulares, longitudes y dependencias personalizadas.
* Controlar tipos MIME y tamaÃ±os de archivo.
* Preparar la API para interoperar con frontends reales.

---

## ğŸ› ï¸ LibrerÃ­as necesarias

FastAPI ya incluye todo lo necesario para este mÃ³dulo.
Solo asegÃºrate de tener `python-multipart`:

```bash
pip install python-multipart
```

Para trabajar con imÃ¡genes opcionalmente:

```bash
pip install pillow
```

---

## ğŸ“¥ Subida de archivos con `UploadFile`

FastAPI maneja archivos grandes de forma eficiente usando `UploadFile` (streaming, sin cargarlo entero en memoria).

### ğŸ“„ Endpoint de ejemplo:

```python
from fastapi import FastAPI, File, UploadFile
import shutil
import os

app = FastAPI()

UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

@app.post("/subir/")
async def subir_archivo(archivo: UploadFile = File(...)):
    ruta = f"{UPLOAD_DIR}/{archivo.filename}"
    with open(ruta, "wb") as buffer:
        shutil.copyfileobj(archivo.file, buffer)
    return {"nombre_archivo": archivo.filename, "tipo": archivo.content_type}
```

ğŸ“Œ Puedes probarlo desde Swagger UI: `/docs`.

---

## ğŸ“‘ Enviar mÃºltiples archivos

```python
@app.post("/multi-subida/")
async def subir_varios(archivos: list[UploadFile] = File(...)):
    nombres = []
    for archivo in archivos:
        ruta = f"{UPLOAD_DIR}/{archivo.filename}"
        with open(ruta, "wb") as buffer:
            shutil.copyfileobj(archivo.file, buffer)
        nombres.append(archivo.filename)
    return {"archivos_guardados": nombres}
```

---

## ğŸ§¾ Procesar formularios HTML con `Form`

```python
from fastapi import Form

@app.post("/login/")
def login(username: str = Form(...), password: str = Form(...)):
    return {"usuario": username}
```

ğŸ“Œ Ideal para integraciones con frontends o clientes `multipart/form-data`.

---

## ğŸ§° ValidaciÃ³n avanzada con Pydantic

```python
from pydantic import BaseModel, EmailStr, Field

class UsuarioRegistro(BaseModel):
    username: str = Field(min_length=3, max_length=20, regex="^[a-zA-Z0-9_]+$")
    email: EmailStr
    password: str = Field(min_length=8)
```

Esto asegura que:

* El nombre de usuario solo contiene letras, nÃºmeros y guiones bajos.
* El email es vÃ¡lido.
* La contraseÃ±a tiene al menos 8 caracteres.

---

## ğŸ§ª ValidaciÃ³n condicional (dependencias personalizadas)

```python
from fastapi import Depends, HTTPException

def verificar_token_admin(token: str = Depends(oauth2_scheme)):
    if token != "token-admin-demo":
        raise HTTPException(status_code=403, detail="No autorizado")
```

Y luego:

```python
@app.post("/solo-admin/")
def solo_admin(dep=Depends(verificar_token_admin)):
    return {"mensaje": "Acceso autorizado"}
```

---

## ğŸ–¼ï¸ Validar archivos por tipo y tamaÃ±o

```python
@app.post("/imagen/")
async def subir_imagen(imagen: UploadFile = File(...)):
    if imagen.content_type not in ["image/png", "image/jpeg"]:
        raise HTTPException(status_code=400, detail="Formato no permitido")
    contenido = await imagen.read()
    if len(contenido) > 2 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="Imagen demasiado grande")
    # Guardar...
```

---

## ğŸ§ª Extra: abrir imagen con Pillow

```python
from PIL import Image

@app.post("/procesar/")
async def procesar_imagen(img: UploadFile = File(...)):
    imagen = Image.open(img.file)
    ancho, alto = imagen.size
    return {"dimensiones": f"{ancho}x{alto}"}
```

---

## âœ… Buenas prÃ¡cticas

* Usa carpetas separadas por tipo (`uploads/images`, `uploads/docs`, etc.).
* Controla los tamaÃ±os mÃ¡ximos de subida.
* No uses el nombre original del archivo sin sanitizar (riesgo de path traversal).
* Genera nombres Ãºnicos: `uuid`, `datetime`, etc.

---

## ğŸ“ Ejercicios prÃ¡cticos

1. Crea un endpoint que permita subir una imagen de perfil y la renombre como `{usuario_id}.jpg`.
2. Crea un endpoint que solo acepte archivos `.pdf` de menos de 5 MB.
3. DiseÃ±a un formulario de registro vÃ­a `Form`, incluyendo validaciÃ³n de campos (usuario, email, contraseÃ±a).
4. Implementa lÃ³gica para borrar archivos antiguos del servidor tras cierto tiempo.

---

Excelente decisiÃ³n. Un backend profesional necesita **tests robustos, automatizados y mantenibles**. Vamos con:

---

# ğŸ§ª MÃ³dulo 4 â€“ Testing profesional en FastAPI con Pytest, mocks y cobertura

---

## ğŸ¯ Objetivos del mÃ³dulo

* Crear tests unitarios y de integraciÃ³n con `pytest`.
* Simular respuestas y componentes externos con `unittest.mock`.
* Medir cobertura de tests con `coverage.py`.
* Usar `TestClient` de FastAPI para probar endpoints.
* Estructurar una carpeta de tests mantenible y clara.

---

## ğŸ“¦ LibrerÃ­as necesarias

```bash
pip install pytest httpx coverage pytest-cov
```

> `httpx` es usado internamente por `TestClient` para simular peticiones HTTP.

---

## ğŸ“ Estructura recomendada

```bash
fastapi_proyecto/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ auth.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ ...
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ test_auth.py
    â”œâ”€â”€ test_usuarios.py
    â””â”€â”€ ...
```

---

## ğŸ§ª `TestClient` para FastAPI

FastAPI ofrece un cliente de pruebas interno:

```python
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_ping():
    response = client.get("/ping")
    assert response.status_code == 200
    assert response.json() == {"mensaje": "pong"}
```

---

## ğŸ“„ `tests/conftest.py` (fixtures reutilizables)

```python
import pytest
from fastapi.testclient import TestClient
from app.main import app
from sqlmodel import Session
from app.database import engine, crear_tablas

@pytest.fixture(scope="module")
def client():
    crear_tablas()
    with TestClient(app) as c:
        yield c
```

---

## âœ… Test de endpoints pÃºblicos

```python
def test_registro_usuario(client):
    res = client.post("/registro", json={
        "username": "usuario1",
        "email": "usuario1@test.com",
        "password": "secreta123"
    })
    assert res.status_code == 200
    data = res.json()
    assert data["username"] == "usuario1"
    assert "id" in data
```

---

## ğŸ” Test de login y rutas protegidas

```python
def test_login_y_token(client):
    res = client.post("/login", data={
        "username": "usuario1",
        "password": "secreta123"
    })
    assert res.status_code == 200
    token = res.json()["access_token"]
    
    # Probar ruta protegida
    headers = {"Authorization": f"Bearer {token}"}
    res2 = client.get("/me", headers=headers)
    assert res2.status_code == 200
    assert res2.json()["username"] == "usuario1"
```

---

## ğŸ§ª Test con mocks (`unittest.mock`)

Simula funciones o dependencias externas (correo, API externa, etc.):

```python
from unittest.mock import patch

@patch("app.auth.crear_token")
def test_login_mock_token(mock_token, client):
    mock_token.return_value = "mocked-token"
    res = client.post("/login", data={"username": "usuario1", "password": "secreta123"})
    assert res.json()["access_token"] == "mocked-token"
```

---

## ğŸ“Š Medir cobertura de tests

```bash
coverage run -m pytest
coverage report -m
```

O con `pytest-cov`:

```bash
pytest --cov=app --cov-report=term-missing
```

---

## ğŸš¨ Casos especiales

### Test con errores esperados

```python
def test_login_contrasena_incorrecta(client):
    res = client.post("/login", data={"username": "usuario1", "password": "mal"})
    assert res.status_code == 400
```

### Test con base de datos aislada

Puedes usar una **base SQLite en memoria** o mocks para evitar modificar la real.

---

## âœ… Buenas prÃ¡cticas

* Nombrar los tests descriptivamente: `test_crear_usuario_valido()`.
* Separar tests por mÃ³dulos.
* No mezclar test unitarios y de integraciÃ³n en un mismo archivo.
* Usar `pytest.mark.parametrize()` para mÃºltiples entradas.
* Aislar dependencias externas con `mock`.

---

## ğŸ“ Ejercicios propuestos

1. AÃ±adir tests para `/admin`, incluyendo errores de acceso.
2. Testear que no se puede registrar con email duplicado.
3. Mockear una funciÃ³n de envÃ­o de correo en el registro.
4. Generar un reporte de cobertura mÃ­nimo del 85%.

---

Excelente decisiÃ³n. Combinaremos **Machine Learning con scikit-learn** y lo integraremos en una API con **FastAPI**, creando una soluciÃ³n lista para producciÃ³n o integraciÃ³n con frontends.

---

# ğŸ¤– MÃ³dulo 5 â€“ Inteligencia Artificial con Python: Scikit-learn + FastAPI

---

## ğŸ¯ Objetivos del mÃ³dulo

* Entrenar un modelo de ML con **scikit-learn**.
* Guardarlo y reutilizarlo con **joblib**.
* Crear una API en **FastAPI** que reciba datos y devuelva predicciones.
* Validar inputs con **Pydantic**.
* Probar predicciones con `curl` o Swagger UI.

---

## ğŸ§  Proyecto: Clasificador de flores (Iris dataset)

Entrenaremos un modelo para clasificar flores (setosa, versicolor, virginica) a partir de 4 caracterÃ­sticas.

---

## ğŸ› ï¸ InstalaciÃ³n de dependencias

```bash
pip install scikit-learn joblib fastapi uvicorn
```

---

## ğŸ“ Estructura del proyecto

```bash
ml_api/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ iris_model.joblib
â”œâ”€â”€ train_model.py
â”œâ”€â”€ main.py
â””â”€â”€ schemas.py
```

---

## ğŸ§ª Paso 1 â€“ Entrenar y guardar el modelo (`train_model.py`)

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import joblib

# Cargar datos
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

# Entrenar modelo
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Guardar modelo
joblib.dump(model, "model/iris_model.joblib")
print("âœ… Modelo guardado.")
```

---

## ğŸ“„ `schemas.py` â€“ ValidaciÃ³n de entrada

```python
from pydantic import BaseModel, Field

class IrisInput(BaseModel):
    sepal_length: float = Field(..., gt=0)
    sepal_width: float = Field(..., gt=0)
    petal_length: float = Field(..., gt=0)
    petal_width: float = Field(..., gt=0)
```

---

## ğŸ“„ `main.py` â€“ Servidor FastAPI con predicciÃ³n

```python
from fastapi import FastAPI
from schemas import IrisInput
import joblib
import numpy as np

# Cargar modelo
model = joblib.load("model/iris_model.joblib")
app = FastAPI(title="API ClasificaciÃ³n de Flores ğŸŒ¸")

@app.post("/predecir/")
def predecir(data: IrisInput):
    features = np.array([[data.sepal_length, data.sepal_width, data.petal_length, data.petal_width]])
    pred = model.predict(features)[0]
    clases = ["setosa", "versicolor", "virginica"]
    return {
        "entrada": data.dict(),
        "clase_predicha": clases[pred]
    }
```

---

## ğŸš€ Ejecutar API

```bash
uvicorn main:app --reload
```

* Abre Swagger UI: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ğŸ§ª Ejemplo de entrada JSON

```json
{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}
```

Respuesta esperada:

```json
{
  "entrada": {
    "sepal_length": 5.1,
    "sepal_width": 3.5,
    "petal_length": 1.4,
    "petal_width": 0.2
  },
  "clase_predicha": "setosa"
}
```

---

## âœ… Buenas prÃ¡cticas

* Aislar el modelo en una carpeta y versiÃ³nalo (`model/iris_model_v1.joblib`).
* Usar `Field(..., gt=0)` para evitar entradas invÃ¡lidas.
* AÃ±adir logging para auditorÃ­a de predicciones.
* Opcional: guardar historiales de entrada/salida en base de datos para revisiÃ³n.

---

## ğŸ“ Ejercicios sugeridos

1. Entrenar un modelo diferente (SVM, KNN) y comparar resultados.
2. AÃ±adir probabilidad de predicciÃ³n (`model.predict_proba()`).
3. Crear un endpoint `/health` que indique si el modelo estÃ¡ cargado correctamente.
4. Implementar autenticaciÃ³n JWT para el endpoint de predicciÃ³n.

---

Perfecto. Entramos ahora en el mundo del **Big Data con Python**, donde aprenderÃ¡s a procesar volÃºmenes masivos de datos con herramientas eficientes como **Pandas, Dask y PySpark**.

---

# ğŸ§  MÃ³dulo 6 â€“ Big Data con Python: Pandas, Dask y PySpark

---

## ğŸ¯ Objetivos del mÃ³dulo

* Comprender las limitaciones de **Pandas** y cuÃ¡ndo usar alternativas.
* Procesar datasets grandes con **Dask**, sin cargar todo en memoria.
* Trabajar con **PySpark** para procesamiento distribuido.
* Aprender a optimizar operaciones, paralelizar y escalar.
* Ejecutar anÃ¡lisis prÃ¡cticos en CSVs o datasets masivos.

---

## ğŸ“¦ Herramientas y librerÃ­as

```bash
pip install pandas dask[complete] pyspark
```

---

## ğŸ“„ Parte 1 â€“ Pandas (procesamiento en memoria)

### âœ… Ideal para:

* Datos pequeÃ±os o medianos (< 2 GB).
* AnÃ¡lisis rÃ¡pido, limpieza, transformaciones.

```python
import pandas as pd

df = pd.read_csv("ventas.csv")

# Limpiar valores nulos
df = df.dropna()

# Agrupar por paÃ­s
resumen = df.groupby("pais")["ventas"].sum().sort_values(ascending=False)
print(resumen.head())
```

ğŸ“‰ Limite: se carga todo el dataset en memoria â†’ problemas con grandes volÃºmenes.

---

## âš™ï¸ Parte 2 â€“ Dask (paralelismo y eficiencia en un solo equipo)

Dask permite trabajar con **DataFrames tipo Pandas**, pero por bloques y en paralelo.

### ğŸš€ Ventajas:

* Usa mÃºltiples nÃºcleos automÃ¡ticamente.
* Maneja datasets mÃ¡s grandes que la RAM.
* API muy parecida a Pandas.

### ğŸ§ª Ejemplo:

```python
import dask.dataframe as dd

df = dd.read_csv("datos_grandes/*.csv")

# Operaciones similares a Pandas
total_por_categoria = df.groupby("categoria")["ventas"].sum().compute()

print(total_por_categoria)
```

### ğŸ§  Notas:

* `.compute()` es necesario para ejecutar las operaciones.
* Dask ejecuta **perezosamente**, construyendo un grafo de tareas.

---

## ğŸ§© Parte 3 â€“ PySpark (procesamiento distribuido estilo Hadoop)

### ğŸ”¥ Ideal para:

* Procesamiento a escala (decenas o cientos de GB).
* Ambientes con clÃºster (Spark, Databricks, EMR, etc.).
* PreparaciÃ³n para entornos empresariales.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Ventas").getOrCreate()

df = spark.read.csv("ventas.csv", header=True, inferSchema=True)

df.groupBy("pais").sum("ventas").orderBy("sum(ventas)", ascending=False).show(5)
```

### ğŸ§  CaracterÃ­sticas:

* Funciona por defecto en modo local (`local[*]`).
* TambiÃ©n trabaja en modo clÃºster (Spark Standalone, YARN, Kubernetes).
* Usa transformaciones *lazy* y optimizaciÃ³n interna.

---

## ğŸ“Š ComparaciÃ³n rÃ¡pida

| Herramienta | Escala | API similar a Pandas  | Paralelismo     | Ideal para          |
| ----------- | ------ | --------------------- | --------------- | ------------------- |
| Pandas      | Baja   | SÃ­                    | No              | Scripts locales     |
| Dask        | Media  | SÃ­                    | SÃ­ (multi-core) | CSVs grandes        |
| PySpark     | Alta   | Similar pero distinta | SÃ­ (clÃºster)    | Empresas, clÃºsteres |

---

## ğŸ“ Ejercicios prÃ¡cticos

1. Cargar y analizar un dataset de +1 millÃ³n de registros con Dask.
2. Comparar el tiempo de ejecuciÃ³n entre Pandas y Dask.
3. Crear una app con PySpark que calcule:

   * Promedio de ventas por regiÃ³n.
   * 10 productos mÃ¡s vendidos.
   * DÃ­as con mayores ventas.
4. Exportar los resultados a Parquet desde Dask o PySpark.

---

## âœ… Buenas prÃ¡cticas

* Para Dask, organiza tus datos por **archivos pequeÃ±os**, no un Ãºnico CSV enorme.
* Usa `Parquet` o `Avro` para mejores lecturas en Big Data.
* Con PySpark, aprovecha `DataFrame API` antes que RDDs.
* Dask se integra bien con FastAPI, Jupyter y Dash.

---

Excelente. Vamos con un mÃ³dulo clave para entornos profesionales: **despliegue automÃ¡tico y profesional con Docker, Uvicorn, Nginx y GitHub Actions**, cubriendo desde la contenerizaciÃ³n hasta la integraciÃ³n continua y la puesta en producciÃ³n.

---

# ğŸš€ MÃ³dulo 7 â€“ CI/CD y despliegue profesional con Docker, Uvicorn, Nginx y GitHub Actions

---

## ğŸ¯ Objetivos del mÃ³dulo

* Contenerizar una aplicaciÃ³n FastAPI con **Docker**.
* Servirla eficientemente en producciÃ³n con **Uvicorn + Gunicorn**.
* Configurar **Nginx** como proxy inverso.
* Automatizar el flujo con **GitHub Actions** para CI/CD.
* Desplegar en un servidor (ej. VPS o contenedor cloud-ready).

---

## ğŸ“ Estructura base del proyecto

```bash
fastapi_proyecto/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ default.conf
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml
â””â”€â”€ requirements.txt
```

---

## ğŸ³ Paso 1 â€“ `Dockerfile` para FastAPI + Uvicorn + Gunicorn

```Dockerfile
FROM python:3.11-slim

# Variables de entorno
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

# Instalar dependencias
WORKDIR /app
COPY requirements.txt .
RUN pip install --upgrade pip && pip install -r requirements.txt

# Copiar el proyecto
COPY . .

# Ejecutar con Gunicorn + Uvicorn
CMD ["gunicorn", "-k", "uvicorn.workers.UvicornWorker", "app.main:app", "--bind", "0.0.0.0:8000"]
```

---

## ğŸ“¦ `requirements.txt` (mÃ­nimo)

```
fastapi
uvicorn[standard]
gunicorn
```

---

## âš™ï¸ Paso 2 â€“ Configurar Nginx como proxy reverso

ğŸ“„ `nginx/default.conf`:

```nginx
server {
    listen 80;
    server_name localhost;

    location / {
        proxy_pass http://web:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

---

## ğŸ§© Paso 3 â€“ `docker-compose.yml` (FastAPI + Nginx)

```yaml
version: '3.8'

services:
  web:
    build: .
    container_name: fastapi_app
    restart: always
    expose:
      - 8000
    volumes:
      - .:/app

  nginx:
    image: nginx:stable
    container_name: fastapi_nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - web
```

---

## ğŸš€ Paso 4 â€“ Construir y ejecutar

```bash
docker-compose up --build
```

Accede a: [http://localhost](http://localhost)

---

## ğŸ”„ Paso 5 â€“ CI/CD con GitHub Actions

ğŸ“„ `.github/workflows/deploy.yml`

```yaml
name: CI/CD FastAPI

on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repo
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run tests
      run: |
        pytest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - name: SSH & Deploy
      uses: appleboy/ssh-action@master
      with:
        host: ${{ secrets.SSH_HOST }}
        username: ${{ secrets.SSH_USER }}
        key: ${{ secrets.SSH_KEY }}
        script: |
          cd /ruta/del/proyecto
          git pull origin main
          docker-compose down
          docker-compose up --build -d
```

> **ğŸ” AsegÃºrate de configurar los `secrets` en GitHub:**
> `SSH_HOST`, `SSH_USER`, `SSH_KEY`

---

## âœ… Buenas prÃ¡cticas

* Usa Gunicorn en producciÃ³n (mejor gestiÃ³n de workers).
* No expongas puertos de backend directamente al pÃºblico.
* Automatiza el testeo en cada `push`.
* Usa certificados HTTPS en Nginx con Let's Encrypt (opcional).
* MantÃ©n tus imÃ¡genes livianas (usa Alpine o `--no-cache`).

---

## ğŸ“ Ejercicios propuestos

1. AÃ±adir un `health check` para Nginx y FastAPI.
2. Configurar despliegue automÃ¡tico tambiÃ©n en push a `develop`.
3. Versionar imÃ¡genes con etiquetas (`image: fastapi_app:v1`).
4. AÃ±adir supervisiÃ³n con Grafana + Prometheus en otra red Docker.

---

Perfecto, vamos a crear un **Proyecto Final Integrador** que reÃºna todo lo aprendido en este plan formativo: **Python avanzado, FastAPI, Inteligencia Artificial, Big Data y Despliegue Profesional.**

---

# ğŸ§© Proyecto Final â€“ Plataforma de predicciÃ³n de abandono de clientes (Churn Predictor)

---

## ğŸ§  Â¿QuÃ© vamos a construir?

Una **plataforma web** con backend en FastAPI, que:

* Reciba informaciÃ³n de clientes.
* Use un modelo de Machine Learning entrenado previamente para predecir si un cliente va a abandonar (churn).
* Guarde los datos y predicciones en un sistema de almacenamiento tipo Big Data (CSV o Parquet).
* Permita consultar estadÃ­sticas agregadas.
* EstÃ© desplegada profesionalmente con Docker + Nginx + GitHub Actions.

---

## ğŸ—‚ï¸ Estructura del proyecto

```bash
churn_predictor/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ model.py             # Carga del modelo
â”‚   â”œâ”€â”€ schemas.py           # ValidaciÃ³n
â”‚   â”œâ”€â”€ routes.py            # Rutas
â”‚   â””â”€â”€ utils.py             # Utilidades
â”œâ”€â”€ ml/
â”‚   â””â”€â”€ churn_model.joblib   # Modelo ML entrenado
â”œâ”€â”€ data/
â”‚   â””â”€â”€ registros.parquet    # Historico de predicciones
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ train_model.py
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ default.conf
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ deploy.yml
```

---

## ğŸ› ï¸ 1. Entrenamiento del modelo (`train_model.py`)

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import joblib

df = pd.read_csv("churn_dataset.csv")  # Incluye campos como edad, ingresos, duraciÃ³n, etc.

X = df.drop(columns=["churn"])
y = df["churn"]

model = RandomForestClassifier()
model.fit(X, y)

joblib.dump(model, "ml/churn_model.joblib")
print("âœ… Modelo entrenado y guardado.")
```

---

## ğŸ§¾ 2. `schemas.py` â€“ Entrada de datos del cliente

```python
from pydantic import BaseModel, Field

class ClienteInput(BaseModel):
    edad: int = Field(gt=17, lt=100)
    ingresos: float = Field(gt=0)
    duracion_meses: int = Field(gt=0)
    uso_app: float = Field(gt=0)
    soporte_llamadas: int = Field(ge=0)
```

---

## ğŸ” 3. `model.py` â€“ Carga del modelo ML

```python
import joblib
import numpy as np

modelo = joblib.load("ml/churn_model.joblib")

def predecir_churn(data: list[float]) -> float:
    return modelo.predict_proba([data])[0][1]
```

---

## ğŸŒ 4. `routes.py` â€“ Rutas API

```python
from fastapi import APIRouter
from schemas import ClienteInput
from model import predecir_churn
import pandas as pd
from pathlib import Path

router = APIRouter()

@router.post("/predecir/")
def predecir(cliente: ClienteInput):
    datos = list(cliente.dict().values())
    prob = predecir_churn(datos)
    
    # Guardar en Parquet
    registro = cliente.dict()
    registro["churn_probabilidad"] = prob
    df = pd.DataFrame([registro])
    path = Path("data/registros.parquet")
    if path.exists():
        df_total = pd.read_parquet(path)
        df_total = pd.concat([df_total, df])
    else:
        df_total = df
    df_total.to_parquet(path, index=False)

    return {"churn_probabilidad": round(prob, 3)}
```

---

## ğŸ–¥ï¸ 5. `main.py` â€“ Montar la app

```python
from fastapi import FastAPI
from routes import router

app = FastAPI(title="PredicciÃ³n de Churn")
app.include_router(router)
```

---

## ğŸ³ 6. `Dockerfile` (igual que antes)

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ğŸŒ 7. `nginx/default.conf` (proxy inverso)

```nginx
server {
    listen 80;

    location / {
        proxy_pass http://web:8000;
    }
}
```

---

## ğŸ“¦ 8. `docker-compose.yml`

```yaml
version: '3.8'

services:
  web:
    build: .
    volumes:
      - .:/app
    expose:
      - 8000

  nginx:
    image: nginx:stable
    ports:
      - "80:80"
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - web
```

---

## âš™ï¸ 9. CI/CD con GitHub Actions (`deploy.yml`)

> Igual al mÃ³dulo anterior, solo cambias ruta del proyecto y servidor destino.

---

## ğŸ§ª 10. Ejemplo de entrada JSON para prueba

```json
{
  "edad": 35,
  "ingresos": 40000,
  "duracion_meses": 12,
  "uso_app": 3.4,
  "soporte_llamadas": 5
}
```

---

## âœ… Extras recomendados

* Agregar autenticaciÃ³n con JWT para proteger el endpoint.
* Crear dashboard de administraciÃ³n con Streamlit o Dash.
* Programar backup automÃ¡tico del archivo `registros.parquet`.
* Analizar el archivo con Dask para mÃ©tricas agregadas.

---

Perfecto, vamos a diseÃ±ar un **dashboard de visualizaciÃ³n interactivo** para nuestro proyecto de predicciÃ³n de churn. Utilizaremos **Streamlit**, que permite construir dashboards modernos con Python de forma rÃ¡pida, simple y personalizable.

---

# ğŸ“Š Frontend/Dashboard â€“ VisualizaciÃ³n de Predicciones con Streamlit

---

## ğŸ¯ Objetivos del dashboard

* Mostrar estadÃ­sticas en tiempo real de predicciones almacenadas.
* Permitir subir nuevos registros CSV o Parquet para analizar.
* Visualizar mÃ©tricas como:

  * Porcentaje de clientes con alta probabilidad de churn.
  * Histograma de probabilidades.
  * AnÃ¡lisis por edad, ingresos, duraciÃ³n, etc.
* Filtro por rangos y campos especÃ­ficos.

---

## ğŸ› ï¸ 1. Instalar dependencias

```bash
pip install streamlit pandas plotly pyarrow
```

---

## ğŸ“„ 2. Estructura del frontend

```bash
dashboard/
â”œâ”€â”€ app.py
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ logo.png (opcional)
â””â”€â”€ data/
    â””â”€â”€ registros.parquet (acceso al histÃ³rico)
```

---

## ğŸ§¾ 3. `app.py` â€“ CÃ³digo base del dashboard

```python
import streamlit as st
import pandas as pd
import plotly.express as px
from pathlib import Path

st.set_page_config(page_title="Dashboard Churn", layout="wide")

st.title("ğŸ“ˆ Dashboard de PredicciÃ³n de Abandono (Churn)")
st.markdown("VisualizaciÃ³n de predicciones generadas por la IA de FastAPI.")

DATA_PATH = Path("data/registros.parquet")

@st.cache_data
def cargar_datos():
    if DATA_PATH.exists():
        return pd.read_parquet(DATA_PATH)
    return pd.DataFrame()

df = cargar_datos()

if df.empty:
    st.warning("No hay datos de predicciones disponibles aÃºn.")
    st.stop()

# --- MÃ©tricas principales
col1, col2, col3 = st.columns(3)
col1.metric("Total registros", len(df))
col2.metric("Prob. Churn promedio", f"{df['churn_probabilidad'].mean():.2f}")
col3.metric("Riesgo Alto (>0.7)", f"{(df['churn_probabilidad'] > 0.7).mean() * 100:.1f}%")

# --- Filtros
st.sidebar.header("ğŸ” Filtros")
edad_min, edad_max = st.sidebar.slider("Edad", 18, 90, (18, 90))
df_filtrado = df[(df["edad"] >= edad_min) & (df["edad"] <= edad_max)]

# --- Histograma
st.subheader("DistribuciÃ³n de probabilidad de churn")
fig1 = px.histogram(df_filtrado, x="churn_probabilidad", nbins=20, color_discrete_sequence=["indianred"])
st.plotly_chart(fig1, use_container_width=True)

# --- RelaciÃ³n por ingresos
st.subheader("RelaciÃ³n entre ingresos y probabilidad de churn")
fig2 = px.scatter(df_filtrado, x="ingresos", y="churn_probabilidad", color="edad", size="duracion_meses",
                  hover_data=["uso_app", "soporte_llamadas"])
st.plotly_chart(fig2, use_container_width=True)

# --- Tabla de detalle
st.subheader("ğŸ“‹ Tabla de registros")
st.dataframe(df_filtrado.sort_values("churn_probabilidad", ascending=False).reset_index(drop=True))
```

---

## ğŸ–¥ï¸ 4. Ejecutar dashboard localmente

```bash
cd dashboard
streamlit run app.py
```

---

## ğŸ’¡ Funcionalidades opcionales a agregar

* ğŸ“¤ Subida manual de archivos nuevos `.parquet` o `.csv` para anÃ¡lisis.
* ğŸ”’ AutenticaciÃ³n bÃ¡sica con contraseÃ±a o token.
* ğŸ“¬ EnvÃ­o de alertas por email si se detectan clientes en riesgo alto.
* ğŸ“Š ExportaciÃ³n de resultados como Excel o PDF.

---

## ğŸ“¦ Despliegue en producciÃ³n

Puedes desplegar este dashboard como contenedor o servidor independiente:

### ğŸ”¹ OpciÃ³n 1: Docker (Dockerfile)

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .
RUN pip install -r requirements.txt

CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

### ğŸ”¹ OpciÃ³n 2: Desplegar en Streamlit Cloud (gratis)

1. Sube el dashboard a GitHub.
2. Ve a [https://streamlit.io/cloud](https://streamlit.io/cloud) y conÃ©ctalo.
3. AÃ±ade `data/registros.parquet` o apunta a la API como backend de datos.

---

Perfecto. Separar el **backend (FastAPI)** y el **dashboard (Streamlit)** en dos servicios distintos es una **arquitectura limpia y escalable**, ideal para entornos profesionales. Vamos a organizar los servicios con **Docker Compose**, cada uno en su contenedor, comunicÃ¡ndose mediante red interna.

---

# ğŸ§± Arquitectura de servicios

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NGINX     â”‚â”€â”€â”€â”€â–¶â”‚ FastAPI     â”‚     â”‚ Streamlit   â”‚
â”‚ (proxy)    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â–²                  â–²
                         â”‚                  â”‚
                  PredicciÃ³n de datos   Lectura Parquet
```

---

# ğŸ“ Estructura del proyecto multi-servicio

```bash
churn_project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â””â”€â”€ churn_model.joblib
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ registros.parquet
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ data/ (montado desde backend)
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ default.conf
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ deploy.yml
```

---

# âš™ï¸ `docker-compose.yml` completo

```yaml
version: "3.9"

services:
  fastapi:
    build: ./backend
    container_name: churn_backend
    volumes:
      - ./backend/data:/app/data
    expose:
      - 8000
    networks:
      - internal

  streamlit:
    build: ./dashboard
    container_name: churn_dashboard
    volumes:
      - ./backend/data:/app/data
    ports:
      - "8501:8501"
    networks:
      - internal

  nginx:
    image: nginx:stable
    container_name: nginx_gateway
    ports:
      - "80:80"
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - fastapi
    networks:
      - internal

networks:
  internal:
    driver: bridge
```

---

# ğŸ§¾ NGINX como proxy inverso (opcional)

ğŸ“„ `nginx/default.conf`:

```nginx
server {
    listen 80;

    location /api/ {
        proxy_pass http://fastapi:8000/;
    }

    location /dashboard/ {
        proxy_pass http://streamlit:8501/;
    }
}
```

Accede luego a:

* API: `http://localhost/api/docs`
* Dashboard: `http://localhost/dashboard/`

---

# ğŸ³ Dockerfile â€“ Backend (`backend/Dockerfile`)

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

# ğŸ³ Dockerfile â€“ Dashboard (`dashboard/Dockerfile`)

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

---

# ğŸ Puesta en marcha

```bash
docker-compose up --build
```

---

# âœ… Ventajas de esta arquitectura

| Aspecto                 | Beneficio                                     |
| ----------------------- | --------------------------------------------- |
| SeparaciÃ³n de servicios | Mejor escalabilidad y mantenimiento           |
| Red compartida          | ComunicaciÃ³n segura entre backend y dashboard |
| VolÃºmenes compartidos   | Dashboard accede a los datos del backend      |
| Preparado para CI/CD    | Cada servicio puede tener su propio flujo     |

---

Perfecto. Vamos a mejorar el **dashboard de visualizaciÃ³n de churn** con:

---

# ğŸ” AutenticaciÃ³n bÃ¡sica

# ğŸ”” Alertas de riesgo alto

# ğŸ“Š EstadÃ­sticas avanzadas

Todo esto manteniendo la simplicidad de Streamlit, pero haciÃ©ndolo mÃ¡s Ãºtil en producciÃ³n o entornos internos de anÃ¡lisis.

---

## âœ… 1. AUTENTICACIÃ“N SIMPLE EN STREAMLIT

No existe login por defecto en Streamlit, pero podemos simular uno con un formulario de inicio de sesiÃ³n controlado por `st.session_state`.

ğŸ“„ En `dashboard/app.py` (parte superior):

```python
import streamlit as st

# Configurar sesiÃ³n
if "autenticado" not in st.session_state:
    st.session_state.autenticado = False

# Formulario de login
if not st.session_state.autenticado:
    st.title("ğŸ”’ Acceso restringido")
    usuario = st.text_input("Usuario")
    clave = st.text_input("ContraseÃ±a", type="password")
    if st.button("Iniciar sesiÃ³n"):
        if usuario == "admin" and clave == "1234":
            st.session_state.autenticado = True
        else:
            st.error("Credenciales invÃ¡lidas")
    st.stop()
```

> ğŸ” Puedes mejorar esto usando `.env` o variables de entorno si se requiere mÃ¡s seguridad.

---

## ğŸ“ˆ 2. ESTADÃSTICAS AVANZADAS

### ğŸ”¹ Porcentaje de riesgo agrupado

```python
riesgo_alto = df[df["churn_probabilidad"] > 0.7]
riesgo_medio = df[(df["churn_probabilidad"] > 0.4) & (df["churn_probabilidad"] <= 0.7)]
riesgo_bajo = df[df["churn_probabilidad"] <= 0.4]

st.subheader("DistribuciÃ³n de riesgo")
col1, col2, col3 = st.columns(3)
col1.metric("ğŸ”´ Alto", f"{len(riesgo_alto)} clientes")
col2.metric("ğŸŸ  Medio", f"{len(riesgo_medio)} clientes")
col3.metric("ğŸŸ¢ Bajo", f"{len(riesgo_bajo)} clientes")
```

### ğŸ”¹ Top 10 clientes con mÃ¡s riesgo

```python
st.subheader("ğŸ” Clientes con mayor probabilidad de abandono")
top_churn = df.sort_values("churn_probabilidad", ascending=False).head(10)
st.table(top_churn)
```

---

## ğŸ“£ 3. ALERTAS AUTOMÃTICAS (por app y visuales)

### Visual: Alerta si hay clientes con churn > 90%

```python
if (df["churn_probabilidad"] > 0.9).any():
    st.error("âš ï¸ Alerta: Â¡Clientes con riesgo extremo detectado!")
```

### Opcional: EnvÃ­o de email con `smtplib` (requiere configuraciÃ³n)

```python
import smtplib
from email.message import EmailMessage

def enviar_alerta():
    msg = EmailMessage()
    msg["Subject"] = "Alerta de Churn"
    msg["From"] = "tudominio@correo.com"
    msg["To"] = "equipo@empresa.com"
    msg.set_content("Se han detectado clientes con churn > 0.9. Revisar el dashboard.")

    with smtplib.SMTP_SSL("smtp.gmail.com", 465) as smtp:
        smtp.login("tudominio@correo.com", "TUPASS")
        smtp.send_message(msg)
```

> âš ï¸ Requiere activar acceso a apps menos seguras si usas Gmail. TambiÃ©n puedes usar servicios externos como Mailgun o SendGrid.

---

## ğŸ“ Adicional: botÃ³n para exportar tabla filtrada

```python
import io

csv = df_filtrado.to_csv(index=False).encode()
st.download_button("â¬‡ï¸ Descargar datos filtrados", csv, file_name="predicciones.csv", mime="text/csv")
```

---

## âœ… Checklist de mejoras implementadas

| Mejora                           | Estado                    |
| -------------------------------- | ------------------------- |
| ğŸ” Login por sesiÃ³n              | âœ… Implementado            |
| ğŸ“£ Alerta por churn > 0.9        | âœ… Visual / Email opcional |
| ğŸ“Š EstadÃ­sticas por categorÃ­a    | âœ… Agregado                |
| ğŸ“„ ExportaciÃ³n de tabla filtrada | âœ… Listo                   |

---

Perfecto. Vamos a implementar un **sistema de roles y permisos bÃ¡sicos en el dashboard**, ideal para separar accesos entre usuarios normales (solo visualizaciÃ³n) y administradores (que puedan ver alertas, descargar datos, etc.).

---

# ğŸ” Sistema de roles y permisos para el dashboard en Streamlit

---

## ğŸ¯ Objetivo

* Incluir autenticaciÃ³n por usuario/contraseÃ±a.
* Asociar un rol a cada usuario: `"admin"` o `"viewer"`.
* Condicionar contenido segÃºn el rol.

---

## ğŸ—‚ï¸ Paso 1: Definir usuarios y roles (puede ser JSON, lista o archivo externo)

ğŸ“„ En la parte superior de `app.py`:

```python
USUARIOS = {
    "admin": {"password": "1234", "rol": "admin"},
    "juan":  {"password": "5678", "rol": "viewer"}
}
```

> âš ï¸ Para producciÃ³n, se recomienda guardar las contraseÃ±as hasheadas (`bcrypt`, `passlib`) o cargarlas desde un `.env` o archivo externo.

---

## ğŸ” Paso 2: Login y gestiÃ³n de sesiÃ³n

```python
if "autenticado" not in st.session_state:
    st.session_state.autenticado = False
    st.session_state.rol = None
    st.session_state.usuario = ""

if not st.session_state.autenticado:
    st.title("ğŸ” Inicio de sesiÃ³n")
    usuario = st.text_input("Usuario")
    clave = st.text_input("ContraseÃ±a", type="password")
    if st.button("Acceder"):
        if usuario in USUARIOS and USUARIOS[usuario]["password"] == clave:
            st.session_state.autenticado = True
            st.session_state.rol = USUARIOS[usuario]["rol"]
            st.session_state.usuario = usuario
            st.success(f"Bienvenido, {usuario} ({st.session_state.rol})")
            st.experimental_rerun()
        else:
            st.error("Credenciales incorrectas")
    st.stop()
```

---

## âš™ï¸ Paso 3: Control de permisos

Luego del login, ya puedes usar `st.session_state.rol` para mostrar o esconder contenido.

### Ejemplo: alertas solo visibles para `"admin"`

```python
if st.session_state.rol == "admin":
    if (df["churn_probabilidad"] > 0.9).any():
        st.error("âš ï¸ Â¡Clientes con riesgo extremo detectado!")

    st.subheader("ğŸ” Top 10 clientes mÃ¡s crÃ­ticos")
    st.table(df.sort_values("churn_probabilidad", ascending=False).head(10))

    # ExportaciÃ³n CSV solo para admins
    csv = df_filtrado.to_csv(index=False).encode()
    st.download_button("â¬‡ï¸ Exportar datos", csv, file_name="datos_churn.csv", mime="text/csv")
else:
    st.info("EstÃ¡s en modo lectura. Algunas funciones estÃ¡n limitadas.")
```

---

## ğŸ§¼ Paso 4: Agregar botÃ³n para cerrar sesiÃ³n

Al final del archivo:

```python
with st.sidebar:
    if st.button("ğŸ”’ Cerrar sesiÃ³n"):
        st.session_state.autenticado = False
        st.session_state.rol = None
        st.experimental_rerun()
```

---

## ğŸ§ª Roles posibles

| Rol    | Permisos                                                             |
| ------ | -------------------------------------------------------------------- |
| admin  | Ver alertas, estadÃ­sticas completas, exportar datos, top 10 clientes |
| viewer | Solo visualizaciÃ³n de grÃ¡ficos, tabla general y filtros              |

Puedes extenderlo con mÃ¡s roles como `analista`, `marketing`, `soporte`, etc., segÃºn necesidad.

---

## âœ… Resultado

Sistema completo de autenticaciÃ³n + autorizaciÃ³n por rol en Streamlit sin dependencias externas, manteniendo todo encapsulado y listo para producciÃ³n interna o despliegue privado.

---

Perfecto. Vamos a generar el `Dockerfile` actualizado del dashboard con autenticaciÃ³n, control de roles y un archivo `.env` para ocultar credenciales. TambiÃ©n lo integramos en el `docker-compose.yml`.

---

# ğŸ³ `dashboard/Dockerfile` actualizado

```Dockerfile
FROM python:3.11-slim

# Variables de entorno
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# Copiar dependencias e instalar
COPY requirements.txt .
RUN pip install --upgrade pip && pip install -r requirements.txt

# Copiar app
COPY . .

# Ejecutar dashboard
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

---

# ğŸ“¦ `dashboard/requirements.txt`

AsegÃºrate de tener:

```txt
streamlit
pandas
plotly
pyarrow
python-dotenv
```

---

# ğŸ” `.env` (en el directorio `dashboard/`)

> âš ï¸ No lo subas a GitHub (aÃ±ade `.env` a `.gitignore`)

```env
USER_ADMIN=admin
PASS_ADMIN=1234
USER_VIEWER=juan
PASS_VIEWER=5678
```

---

# ğŸ“„ `dashboard/app.py` â€“ Leer variables desde `.env`

### Paso 1: Importar y cargar

```python
import os
from dotenv import load_dotenv

load_dotenv()

USUARIOS = {
    os.getenv("USER_ADMIN"): {"password": os.getenv("PASS_ADMIN"), "rol": "admin"},
    os.getenv("USER_VIEWER"): {"password": os.getenv("PASS_VIEWER"), "rol": "viewer"}
}
```

> Puedes agregar mÃ¡s usuarios si lo deseas (usa otro `.env` para entorno real).

---

# ğŸ”— `docker-compose.yml` â€“ IntegraciÃ³n final

Agrega o edita el bloque del servicio `streamlit`:

```yaml
streamlit:
  build: ./dashboard
  container_name: churn_dashboard
  volumes:
    - ./backend/data:/app/data
    - ./dashboard/.env:/app/.env
  ports:
    - "8501:8501"
  networks:
    - internal
  environment:
    - STREAMLIT_SERVER_HEADLESS=true
```

---

# âœ… Comandos para ejecutar

```bash
docker-compose up --build
```

Accede a:

* Dashboard: `http://localhost:8501`
* Se pedirÃ¡ usuario y contraseÃ±a (ej: `admin / 1234`)

---

# ğŸ“Œ Mejores prÃ¡cticas recomendadas

* Cifra las contraseÃ±as con `bcrypt` para producciÃ³n.
* Usa HTTPS (Letâ€™s Encrypt o proxy inverso con NGINX).
* Administra usuarios desde una base de datos o API backend si el sistema crece.
* AÃ±ade logging de accesos o intentos fallidos.

---

Perfecto, Mario. Vamos a reemplazar el acceso del dashboard a archivos Parquet por **peticiones directas al backend FastAPI**, lo que mejora la escalabilidad, seguridad, sincronizaciÃ³n y estructura de microservicios reales.

---

# ğŸ”„ Reemplazar Parquet por API REST â€“ ConexiÃ³n directa FastAPI + Streamlit

---

## ğŸ¯ Objetivo

* El backend FastAPI expone una API REST para:

  * Crear predicciones (ya existente).
  * Consultar predicciones realizadas (nuevo endpoint).
* El dashboard Streamlit consulta directamente estas rutas vÃ­a HTTP.
* El almacenamiento (en disco o en DB) queda en manos del backend.

---

## ğŸ“¦ 1. **Modificar el Backend FastAPI** para exponer predicciones

ğŸ“„ `routes.py` â€“ AÃ±ade nuevo endpoint:

```python
from fastapi import APIRouter
from model import predecir_churn
from schemas import ClienteInput
import pandas as pd
from pathlib import Path
from fastapi.responses import JSONResponse

router = APIRouter()
DATA_PATH = Path("data/registros.parquet")

@router.post("/predecir/")
def predecir(cliente: ClienteInput):
    datos = list(cliente.dict().values())
    prob = predecir_churn(datos)

    # Guardar resultado
    registro = cliente.dict()
    registro["churn_probabilidad"] = prob
    df = pd.DataFrame([registro])
    
    if DATA_PATH.exists():
        df_total = pd.read_parquet(DATA_PATH)
        df_total = pd.concat([df_total, df])
    else:
        df_total = df
    df_total.to_parquet(DATA_PATH, index=False)

    return {"churn_probabilidad": round(prob, 3)}

@router.get("/predicciones/")
def obtener_predicciones():
    if DATA_PATH.exists():
        df = pd.read_parquet(DATA_PATH)
        return JSONResponse(content=df.to_dict(orient="records"))
    return JSONResponse(content=[])
```

---

## ğŸŒ 2. **Conectar el Dashboard Streamlit a la API**

ğŸ“„ `dashboard/app.py` â€“ Reemplaza `cargar_datos()`:

```python
import requests

API_URL = "http://fastapi:8000"  # Cambiar si accedes desde fuera del contenedor

@st.cache_data(ttl=60)
def cargar_datos():
    try:
        r = requests.get(f"{API_URL}/predicciones/")
        if r.status_code == 200:
            return pd.DataFrame(r.json())
    except Exception as e:
        st.error(f"Error al conectarse con el backend: {e}")
    return pd.DataFrame()
```

---

## ğŸ§ª Probar localmente

Accede a `http://localhost:8501`, y el dashboard ya mostrarÃ¡ los datos actualizados en tiempo real directamente desde el backend vÃ­a API, sin leer archivos directamente.

---

## ğŸ³ 3. Ajuste en `docker-compose.yml` para acceso entre servicios

Ya que Streamlit y FastAPI comparten red Docker, **usa `fastapi` como hostname** (es el nombre del servicio):

```yaml
services:
  fastapi:
    container_name: fastapi
    ...

  streamlit:
    container_name: dashboard
    environment:
      - API_URL=http://fastapi:8000
    ...
```

Si quieres usar una variable en lugar de hardcodear `http://fastapi:8000`, puedes hacer:

```python
import os
API_URL = os.getenv("API_URL", "http://fastapi:8000")
```

---

## ğŸ›¡ï¸ Ventajas clave de usar la API:

| Aspecto           | Archivos locales                | API REST (nuevo enfoque)            |
| ----------------- | ------------------------------- | ----------------------------------- |
| ğŸ”„ SincronizaciÃ³n | Manual o por volumen            | Siempre actualizados                |
| ğŸ” Seguridad      | Cualquier app puede leer        | Control de acceso en backend        |
| ğŸ“ˆ Escalabilidad  | DifÃ­cil en mÃºltiples instancias | FÃ¡cil de extender (BD, cache, etc.) |
| ğŸ§± Modularidad    | Acoplamiento fuerte             | Microservicios reales               |

---

## ğŸš€ PrÃ³ximos pasos opcionales

1. AÃ±adir paginaciÃ³n, filtros y ordenamiento al endpoint `/predicciones/`.
2. Reemplazar Parquet por base de datos (SQLite, PostgreSQL o MongoDB).
3. Incluir un `/predicciones/stats` que devuelva mÃ©tricas listas para el dashboard.
4. AÃ±adir cache o fondo de sincronizaciÃ³n (ej: Redis, Celery).

---

Perfecto, Mario. Vamos a migrar el almacenamiento de predicciones de **archivos Parquet a una base de datos relacional**, especÃ­ficamente **SQLite** (por simplicidad en local o desarrollo), pero fÃ¡cilmente escalable a PostgreSQL o MySQL.

---

# ğŸ” MigraciÃ³n de almacenamiento de Parquet a Base de Datos (SQLite)

---

## ğŸ”§ Objetivo

* Usar **SQLAlchemy** + **SQLite** para guardar y consultar las predicciones.
* Modificar el backend FastAPI para almacenar y exponer datos desde la DB.
* El dashboard Streamlit seguirÃ¡ consumiendo los datos desde la API.

---

## âš™ï¸ Paso 1: AÃ±adir dependencias al backend

ğŸ“„ `backend/requirements.txt`:

```txt
fastapi
uvicorn
pydantic
pandas
scikit-learn
joblib
sqlalchemy
```

---

## ğŸ“¦ Paso 2: Configurar la base de datos con SQLAlchemy

ğŸ“„ `app/db.py`:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_URL = "sqlite:///./data/churn.db"

engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()
```

---

ğŸ“„ `app/models_db.py` â€“ Definir la tabla

```python
from sqlalchemy import Column, Integer, Float
from db import Base

class Prediccion(Base):
    __tablename__ = "predicciones"

    id = Column(Integer, primary_key=True, index=True)
    edad = Column(Integer)
    ingresos = Column(Float)
    duracion_meses = Column(Integer)
    uso_app = Column(Float)
    soporte_llamadas = Column(Integer)
    churn_probabilidad = Column(Float)
```

---

ğŸ“„ En `main.py` â€“ Crear la tabla si no existe

```python
from db import Base, engine

Base.metadata.create_all(bind=engine)
```

---

## ğŸš€ Paso 3: Modificar rutas para usar la base de datos

ğŸ“„ `routes.py`:

```python
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from db import SessionLocal
from models_db import Prediccion
from schemas import ClienteInput
from model import predecir_churn

router = APIRouter()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@router.post("/predecir/")
def predecir(cliente: ClienteInput, db: Session = Depends(get_db)):
    datos = list(cliente.dict().values())
    prob = predecir_churn(datos)

    pred = Prediccion(**cliente.dict(), churn_probabilidad=prob)
    db.add(pred)
    db.commit()
    db.refresh(pred)

    return {"churn_probabilidad": round(prob, 3)}

@router.get("/predicciones/")
def obtener_predicciones(db: Session = Depends(get_db)):
    registros = db.query(Prediccion).all()
    return [r.__dict__ for r in registros]
```

> ğŸ” Puedes limpiar `__dict__` antes de devolverlo si usas modelos con campos privados.

---

## âœ… Resultado: Backend funcionando con base de datos

Ya puedes usar `/predecir/` y `/predicciones/` totalmente sobre SQLite, sin Parquet.

---

## ğŸ§ª Paso 4: El dashboard no necesita cambiar

El dashboard seguirÃ¡ haciendo:

```python
r = requests.get("http://fastapi:8000/predicciones/")
df = pd.DataFrame(r.json())
```

Solo que ahora el backend devuelve los datos directamente desde la DB.

---

## ğŸ“¦ Bonus: Persistencia con Docker

En `docker-compose.yml`, agrega volumen al servicio `fastapi`:

```yaml
fastapi:
  ...
  volumes:
    - ./backend/data:/app/data
```

AsÃ­ se persistirÃ¡ `churn.db`.

---

## ğŸ”„ MigraciÃ³n completada

| Antes (Parquet)             | Ahora (Base de datos SQLite)      |
| --------------------------- | --------------------------------- |
| Lectura/escritura manual    | ORM con SQLAlchemy                |
| Datos en archivo plano      | Consultas filtradas, persistentes |
| Escalabilidad limitada      | FÃ¡cil de migrar a PostgreSQL      |
| No relaciones ni integridad | Total control y normalizaciÃ³n     |

---

Perfecto, Mario. Migrar de SQLite a **PostgreSQL** es un paso esencial para llevar tu backend a producciÃ³n real: mÃ¡s robustez, concurrencia, control de acceso, backups, rendimiento, etc.

Vamos a hacerlo paso a paso.

---

# ğŸ˜ MigraciÃ³n de SQLite a PostgreSQL con SQLAlchemy en FastAPI

---

## ğŸ”§ 1. Cambios en dependencias

ğŸ“„ `backend/requirements.txt`

AsegÃºrate de incluir:

```txt
psycopg2-binary
sqlalchemy
```

> âš ï¸ Usa `psycopg2-binary` solo en desarrollo o contenedores. Para producciÃ³n, considera `psycopg2`.

---

## ğŸ“¦ 2. Crear contenedor PostgreSQL con Docker Compose

ğŸ“„ `docker-compose.yml` â€“ AÃ±adir servicio `postgres`

```yaml
services:
  postgres:
    image: postgres:16
    container_name: churn_db
    environment:
      POSTGRES_USER: mario
      POSTGRES_PASSWORD: strongpassword
      POSTGRES_DB: churn_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - internal
    ports:
      - "5432:5432"

  fastapi:
    build: ./backend
    container_name: churn_backend
    depends_on:
      - postgres
    environment:
      - DATABASE_URL=postgresql://mario:strongpassword@postgres:5432/churn_db
    networks:
      - internal
    volumes:
      - ./backend:/app

volumes:
  postgres_data:

networks:
  internal:
    driver: bridge
```

---

## âš™ï¸ 3. Cambiar la conexiÃ³n en `db.py`

ğŸ“„ `backend/app/db.py`:

```python
import os
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://mario:strongpassword@localhost:5432/churn_db")

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False)
Base = declarative_base()
```

> âœ… AsegÃºrate de que `DATABASE_URL` estÃ© configurado como variable de entorno.

---

## ğŸ“„ 4. Crear la base de datos al iniciar

ğŸ“„ `main.py`:

```python
from app.db import Base, engine

Base.metadata.create_all(bind=engine)
```

Esto crearÃ¡ la tabla `predicciones` al iniciar el contenedor.

---

## ğŸ§ª 5. Probar conexiÃ³n y migraciÃ³n

Ejecuta:

```bash
docker-compose up --build
```

Y revisa que en los logs de FastAPI veas algo como:

```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

Y si accedes a `http://localhost:8000/predicciones/` devuelve un array vacÃ­o `[]`, entonces Â¡ya estÃ¡s usando PostgreSQL!

---

## ğŸ—‚ï¸ 6. Opcional: inspeccionar desde tu host

Puedes conectarte a PostgreSQL desde tu mÃ¡quina con herramientas como:

```bash
psql -h localhost -U mario -d churn_db
```

---

## âœ… Resultado

| Aspecto              | SQLite                 | PostgreSQL                 |
| -------------------- | ---------------------- | -------------------------- |
| Persistencia         | Archivos locales `.db` | Volumen Docker / servidor  |
| Concurrencia         | Limitada               | Alta y estable             |
| Escalabilidad        | Baja                   | Ideal para producciÃ³n      |
| Compatibilidad cloud | Pobre                  | Total (AWS RDS, Supabaseâ€¦) |

---

## ğŸ›¡ï¸ Seguridad recomendada

* Usa variables de entorno para `DATABASE_URL` (ya estÃ¡ preparado).
* Para producciÃ³n, activa conexiones seguras (SSL).
* AÃ±ade backups automÃ¡ticos y logs.

---

Perfecto, Mario. AÃ±adir **filtros y paginaciÃ³n al endpoint `/predicciones/`** es un paso esencial para optimizar el rendimiento y la experiencia del dashboard, especialmente cuando la base de datos crezca.

---

# ğŸ“¥ Mejora del endpoint `/predicciones/` con filtros y paginaciÃ³n

---

## âœ… Objetivo

* Filtrar por:

  * Rango de edad
  * Rango de probabilidad de churn
* AÃ±adir paginaciÃ³n:

  * `limit` (nÂº de registros por pÃ¡gina)
  * `offset` (desde quÃ© Ã­ndice)
* Opcional: ordenamiento por columnas

---

## ğŸ”§ Paso 1: Actualizar el endpoint en `routes.py`

ğŸ“„ `backend/app/routes.py`

```python
from fastapi import APIRouter, Depends, Query
from sqlalchemy.orm import Session
from db import SessionLocal
from models_db import Prediccion

router = APIRouter()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@router.get("/predicciones/")
def obtener_predicciones(
    db: Session = Depends(get_db),
    edad_min: int = Query(0, ge=0),
    edad_max: int = Query(120, le=120),
    churn_min: float = Query(0.0, ge=0.0, le=1.0),
    churn_max: float = Query(1.0, ge=0.0, le=1.0),
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0)
):
    query = db.query(Prediccion).filter(
        Prediccion.edad >= edad_min,
        Prediccion.edad <= edad_max,
        Prediccion.churn_probabilidad >= churn_min,
        Prediccion.churn_probabilidad <= churn_max
    )

    total = query.count()
    resultados = query.offset(offset).limit(limit).all()

    return {
        "total": total,
        "limit": limit,
        "offset": offset,
        "resultados": [r.__dict__ for r in resultados]
    }
```

---

## ğŸ”„ Paso 2: Actualizar el dashboard para consumir los filtros

ğŸ“„ `dashboard/app.py`

```python
# Filtros en la barra lateral
st.sidebar.header("Filtros")
edad_min, edad_max = st.sidebar.slider("Edad", 18, 90, (18, 90))
churn_min, churn_max = st.sidebar.slider("Churn %", 0.0, 1.0, (0.0, 1.0))
limit = st.sidebar.selectbox("Resultados por pÃ¡gina", [10, 25, 50, 100])
offset = st.sidebar.number_input("Offset", min_value=0, step=limit)

@st.cache_data(ttl=60)
def cargar_datos():
    params = {
        "edad_min": edad_min,
        "edad_max": edad_max,
        "churn_min": churn_min,
        "churn_max": churn_max,
        "limit": limit,
        "offset": offset
    }
    r = requests.get(f"{API_URL}/predicciones/", params=params)
    data = r.json()
    return pd.DataFrame(data["resultados"]), data["total"]

df, total = cargar_datos()

st.caption(f"Mostrando {len(df)} de {total} registros filtrados")
```

---

## ğŸ§ª Ejemplo de uso en Postman o navegador

```
GET http://localhost:8000/predicciones/?edad_min=30&edad_max=50&churn_min=0.7&limit=25&offset=0
```

---

## ğŸ§± Resultado final

âœ” Backend ahora permite:

* Filtros por rango de edad y churn
* PaginaciÃ³n con `limit` y `offset`
* Total de registros disponibles con los filtros

âœ” Dashboard puede:

* Ofrecer navegaciÃ³n por pÃ¡ginas
* Mostrar mÃ©tricas resumidas
* Mejorar rendimiento (consulta solo lo necesario)

---

## ğŸ§© Posibles mejoras futuras

* âœ… Ordenar por columnas (`?order_by=churn_probabilidad&desc=true`)
* ğŸ” Buscar por cliente (si aÃ±ades campo de nombre, email, etc.)
* ğŸš€ Endpoint `/predicciones/stats/` para mÃ©tricas agregadas

---

Perfecto, Mario. Vamos a proteger tu **API FastAPI** con **OAuth2 y JWT (JSON Web Tokens)**, el estÃ¡ndar mÃ¡s usado en autenticaciÃ³n moderna para APIs.

Esto permite:

* Proteger endpoints sensibles.
* AutenticaciÃ³n sin sesiones.
* Soporte para autorizaciÃ³n por roles (mÃ¡s adelante).

---

# ğŸ” Seguridad en FastAPI con OAuth2 + JWT

---

## ğŸ§± Arquitectura general

```
[Cliente Front / Dashboard]
          â”‚
     POST /login   â†’  recibe token JWT
          â”‚
     GET /predicciones/ con Header:
     Authorization: Bearer <token>
```

---

## âœ… Paso 1: Instalar dependencias

ğŸ“„ `backend/requirements.txt`

```txt
python-jose[cryptography]
passlib[bcrypt]
```

> `jose`: Para generar/verificar JWT
> `passlib`: Para hashear contraseÃ±as

---

## ğŸ§© Paso 2: Estructura de seguridad

ğŸ“„ `security.py`

```python
from datetime import datetime, timedelta
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi import HTTPException, status, Depends
from fastapi.security import OAuth2PasswordBearer
from sqlalchemy.orm import Session
from db import SessionLocal

# Clave secreta (usa os.getenv en producciÃ³n)
SECRET_KEY = "clave-super-secreta"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/login")

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verificar_password(plain, hashed):
    return pwd_context.verify(plain, hashed)

def hashear_password(password):
    return pwd_context.hash(password)

def crear_token(datos: dict, expires_delta: timedelta = None):
    to_encode = datos.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def obtener_usuario(db: Session, username: str):
    # Ficticio por ahora. Puedes conectar a tabla de usuarios.
    if username == "admin":
        return {"username": "admin", "hashed_password": hashear_password("1234"), "rol": "admin"}
    return None

def autenticar_usuario(db: Session, username: str, password: str):
    user = obtener_usuario(db, username)
    if not user or not verificar_password(password, user["hashed_password"]):
        return None
    return user

def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username = payload.get("sub")
        if username is None:
            raise HTTPException(status_code=401)
        return {"username": username}
    except JWTError:
        raise HTTPException(status_code=401)
```

---

## ğŸ”‘ Paso 3: Endpoint `/login`

ğŸ“„ `routes.py` o `auth.py`:

```python
from fastapi import APIRouter, Depends, Form
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from security import autenticar_usuario, crear_token, get_current_user
from db import SessionLocal

router = APIRouter()

@router.post("/login")
def login(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(SessionLocal)):
    user = autenticar_usuario(db, form_data.username, form_data.password)
    if not user:
        raise HTTPException(status_code=400, detail="Credenciales invÃ¡lidas")
    
    token = crear_token({"sub": user["username"]})
    return {"access_token": token, "token_type": "bearer"}
```

---

## ğŸ›¡ï¸ Paso 4: Proteger endpoints con `Depends`

```python
@router.get("/predicciones/")
def obtener_predicciones(
    user=Depends(get_current_user),
    db: Session = Depends(get_db),
    ...
):
    # Solo usuarios autenticados acceden
    ...
```

---

## ğŸ§ª Prueba bÃ¡sica con `curl` o Postman

```bash
# Obtener token
curl -X POST http://localhost:8000/login -d "username=admin&password=1234" -H "Content-Type: application/x-www-form-urlencoded"

# Usar token en GET
curl -H "Authorization: Bearer <token>" http://localhost:8000/predicciones/
```

---

## ğŸ“˜ DocumentaciÃ³n automÃ¡tica con `/docs`

FastAPI genera automÃ¡ticamente:

* Formulario de login en `/docs`
* Uso de token para los endpoints protegidos

---

## ğŸ”„ El dashboard Streamlit tambiÃ©n puede usar token

En `dashboard/app.py`, puedes hacer:

```python
TOKEN = "eyJhbGciOi..."  # Obtenido desde login
headers = {"Authorization": f"Bearer {TOKEN}"}
r = requests.get(API_URL + "/predicciones/", headers=headers, params=params)
```

> O automatiza el login con `requests.post('/login', data={...})`

---

## âœ… Ventajas de este sistema

| CaracterÃ­stica          | Valor                       |
| ----------------------- | --------------------------- |
| ğŸ” Seguridad moderna    | JWT, OAuth2 estÃ¡ndar        |
| ğŸ§© Escalable            | Puedes aÃ±adir roles, scopes |
| ğŸ”„ IntegraciÃ³n sencilla | Compatible con frontend     |
| ğŸ“‹ Documentado          | Swagger `/docs` lo detecta  |

---

Â¡Perfecto, Mario! Vamos a preparar el **despliegue completo del sistema**: backend FastAPI con PostgreSQL + dashboard Streamlit + autenticaciÃ³n, todo gestionado con Docker y Docker Compose, listo para producciÃ³n.

---

# ğŸš€ Despliegue completo en servidor (VPS / Cloud)

---

## ğŸ§± Arquitectura final

```
          INTERNET
              â”‚
        [ Reverse Proxy ]
              â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                            â”‚
 â”‚     FastAPI (API)         â”‚  â† Backend protegido con JWT
 â”‚  + PostgreSQL (DB)        â”‚
 â”‚                            â”‚
 â”‚   Streamlit Dashboard      â”‚  â† Usa la API vÃ­a HTTP
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Requisitos mÃ­nimos

* Un **VPS con Ubuntu (22.04 o superior)** o servicios como DigitalOcean, AWS EC2, Vultr, etc.
* **Docker** y **Docker Compose v2** instalados.
* Un dominio (opcional, recomendado).

---

## ğŸ“ Estructura de proyecto recomendada

```
proyecto/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ default.conf
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env
```

---

## ğŸ” Paso 1: Reverse proxy con Nginx + HTTPS

ğŸ“„ `nginx/default.conf`:

```nginx
server {
    listen 80;
    server_name tu-dominio.com;

    location /api/ {
        proxy_pass http://fastapi:8000/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    location / {
        proxy_pass http://streamlit:8501/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

---

## ğŸ“¦ Paso 2: `docker-compose.yml` de producciÃ³n

```yaml
version: '3.9'

services:
  postgres:
    image: postgres:16
    restart: always
    environment:
      POSTGRES_USER: mario
      POSTGRES_PASSWORD: strongpassword
      POSTGRES_DB: churn_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - internal

  fastapi:
    build: ./backend
    container_name: churn_backend
    environment:
      - DATABASE_URL=postgresql://mario:strongpassword@postgres:5432/churn_db
    depends_on:
      - postgres
    networks:
      - internal

  streamlit:
    build: ./dashboard
    container_name: churn_dashboard
    environment:
      - API_URL=http://fastapi:8000
    depends_on:
      - fastapi
    networks:
      - internal

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - fastapi
      - streamlit
    networks:
      - internal

volumes:
  postgres_data:

networks:
  internal:
    driver: bridge
```

---

## ğŸŒ Paso 3: (opcional) AÃ±adir HTTPS con Let's Encrypt (recomendado)

Usa [Caddy](https://caddyserver.com) o [nginx-proxy + Let's Encrypt Companion](https://github.com/nginx-proxy/acme-companion) para generaciÃ³n automÃ¡tica de certificados.

O bien instala Certbot manualmente:

```bash
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d tu-dominio.com
```

---

## ğŸ§ª Paso 4: Desplegar en servidor

1. Copia el proyecto completo al VPS:

```bash
scp -r proyecto/ mario@tu-vps:/home/mario/
```

2. Entra al VPS y ejecuta:

```bash
cd proyecto/
docker-compose up --build -d
```

3. Accede:

* Dashboard: `http://tu-dominio.com/`
* API docs: `http://tu-dominio.com/api/docs`

---

## ğŸ§¼ Paso 5: Mantenimiento y seguridad

* âœ… Crea usuarios reales y controla roles.
* ğŸ” Cifra tokens y contraseÃ±as (usa `.env` para la SECRET\_KEY).
* ğŸ”„ AÃ±ade backup automÃ¡tico de base de datos.
* ğŸ“ˆ Monitoriza con herramientas como UptimeRobot o Grafana.
* ğŸ”„ Usa `watchtower` para actualizaciones automÃ¡ticas de contenedores si quieres CI/CD simple.

---

## âœ… Resultado: sistema de producciÃ³n completo

| Componente         | Estado                             |
| ------------------ | ---------------------------------- |
| ğŸ”§ Backend FastAPI | âœ… JWT + PostgreSQL                 |
| ğŸ“Š Dashboard UI    | âœ… Protegido por rol + API REST     |
| ğŸ§± Infraestructura | âœ… Dockerizada, modular y escalable |
| ğŸŒ Despliegue Web  | âœ… VÃ­a Nginx con opciÃ³n de HTTPS    |

---

















